# This will search your entire Google Drive for the file
for root, dirs, files in os.walk("/content/drive/MyDrive/"):
    for file in files:
        if file == "Benign-Monday-no-metadata.parquet":
            full_path = os.path.join(root, file)
            print(f"FOUND IT! Copy this path: {full_path}")

# Replace the path below with the one you found in Step 2
path = '/content/drive/MyDrive/CICIDS2017/Benign-Monday-no-metadata.parquet'

# Load the data
df = pd.read_parquet(path)

# Show the first 5 rows to confirm it worked
print("Dataset Loaded Successfully!")
print(f"Total Rows: {len(df)}")
df.head()

print(df['Label'].value_counts())

# Load an attack file (example)
# df_attack = pd.read_parquet('/content/drive/MyDrive/Friday-PortScan.parquet')

# Combine them
# df_total = pd.concat([df, df_attack], axis=0)

import os

# This looks through your Drive to find where the Parquet files actually are
file_paths = {}
for root, dirs, files in os.walk("/content/drive/MyDrive/"):
    for file in files:
        if file.endswith(".parquet"):
            file_paths[file] = os.path.join(root, file)

# Print what was found
if not file_paths:
    print("Zero parquet files found. Double check your Drive upload!")
else:
    print(f"Found {len(file_paths)} files:")
    for name, path in file_paths.items():
        print(f"File: {name} -> Path: {path}")

all_data = []

# Loop through the files found in your previous step
for name, path in file_paths.items():
    print(f"Loading and cleaning: {name}...")
    temp_df = pd.read_parquet(path)

    # Cleaning: Handle infinity and empty values immediately to save RAM
    temp_df.replace([np.inf, -np.inf], np.nan, inplace=True)
    temp_df.dropna(inplace=True)

    all_data.append(temp_df)

# Combine everything
df_all = pd.concat(all_data, axis=0, ignore_index=True)

print("\n--- MERGE COMPLETE ---")
print(f"Total rows in dataset: {len(df_all):,}")
print("\nThreat Categories found:")
print(df_all['Label'].value_counts())

# Clean up the names of the attacks
df_all['Label'] = df_all['Label'].str.replace('', '-', regex=False)
print(df_all['Label'].unique())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# 1. Separate Features (X) and Target (y)
X = df_all.drop('Label', axis=1)
y = df_all['Label']

# 2. Convert text labels to numbers
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# 3. Stratified Split (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"Training set size: {len(X_train)}")
print(f"Testing set size: {len(X_test)}")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Separate Benign and Attacks
df_benign = df_all[df_all['Label'] == 'Benign']
df_attacks = df_all[df_all['Label'] != 'Benign']

# Take only 10% of Benign data
df_benign_reduced = df_benign.sample(frac=0.1, random_state=42)

# Combine them back
df_balanced = pd.concat([df_benign_reduced, df_attacks], axis=0)

print(f"New Dataset Size: {len(df_balanced)}")
print(df_balanced['Label'].value_counts())

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Training with 50 trees is usually enough for high accuracy in CICIDS2017
rf_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)

print("Training...")
rf_model.fit(X_train_scaled, y_train)

# Test
y_pred = rf_model.predict(X_test_scaled)

print("\n--- RESULTS ---")
print(classification_report(y_test, y_pred, target_names=le.classes_))

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Generate the matrix
cm = confusion_matrix(y_test, y_pred)

# Plotting
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix: Threat Detection')
plt.show()

print(data.columns)

from sklearn.preprocessing import LabelEncoder

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()

model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))

# Multi-class output (number of unique labels)
model.add(Dense(len(set(y_train)), activation='softmax'))

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint = ModelCheckpoint(
    '/content/drive/MyDrive/ANN_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    callbacks=[checkpoint]
)

loss, acc = model.evaluate(X_test, y_test)
print("ANN Test Accuracy:", acc)

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

print(classification_report(y_test, y_pred_classes))
print(confusion_matrix(y_test, y_pred_classes))

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('ANN Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Predict
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(14, 10))

sns.heatmap(cm,
            annot=True,
            fmt="d",
            cmap="Blues",      # professional white-blue theme
            linewidths=0.5,
            linecolor='black')

plt.title("ANN Confusion Matrix", fontsize=16)
plt.ylabel("Actual Label", fontsize=12)
plt.xlabel("Predicted Label", fontsize=12)

plt.show()

# Reshape for 1D CNN
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn  = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input

model_cnn = Sequential()
model_cnn.add(Input(shape=(X_train.shape[1], 1)))  # <- Use Input layer
model_cnn.add(Conv1D(64, 3, activation='relu'))
model_cnn.add(MaxPooling1D(2))
model_cnn.add(Conv1D(32, 3, activation='relu'))
model_cnn.add(MaxPooling1D(2))
model_cnn.add(Flatten())
model_cnn.add(Dense(64, activation='relu'))
model_cnn.add(Dense(len(set(y_train)), activation='softmax'))

# X_train and X_test currently: (num_samples, num_features)
X_train_cnn = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test_cnn  = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense

model_cnn = Sequential()
model_cnn.add(Input(shape=(X_train.shape[1], 1)))  # Proper input layer
model_cnn.add(Conv1D(64, 3, activation='relu'))
model_cnn.add(MaxPooling1D(2))
model_cnn.add(Conv1D(32, 3, activation='relu'))
model_cnn.add(MaxPooling1D(2))
model_cnn.add(Flatten())
model_cnn.add(Dense(64, activation='relu'))
model_cnn.add(Dense(len(set(y_train)), activation='softmax'))

model_cnn.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint_cnn = ModelCheckpoint(
    '/content/drive/MyDrive/Final_Colab_Code/CNN_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

history_cnn = model_cnn.fit(
    X_train_cnn, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    callbacks=[checkpoint_cnn]
)

from tensorflow.keras.models import load_model

model_cnn = load_model('/content/drive/MyDrive/Final_Colab_Code/CNN_model.h5')
print("Best CNN model loaded from Drive")

loss, acc = model_cnn.evaluate(X_test_cnn, y_test)
print("Final CNN Test Accuracy:", acc)

y_pred = model_cnn.predict(X_test_cnn)
y_pred_classes = np.argmax(y_pred, axis=1)


labels = le.classes_     # if you used LabelEncoder

plt.figure(figsize=(16,12))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels,
            yticklabels=labels)

plt.title("CNN Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("Actual Label")

plt.xticks(rotation=90)
plt.yticks(rotation=0)

plt.show()
